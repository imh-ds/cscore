% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/score_discriminant.R
\name{discriminant_score}
\alias{discriminant_score}
\title{Calculate Discriminant-Weighted Composite Scores}
\usage{
discriminant_score(
  data = .,
  composite_list,
  composite_model = NULL,
  weight = c("irt", "pca", "glm"),
  pred_type = c("glm", "rf"),
  item_type = NULL,
  pmm_k = 5,
  maxiter = 10,
  verbose = 0,
  digits = 3,
  alpha = 0.5,
  nfolds = 10,
  ntrees = 100,
  importance = c("permutation", "impurity"),
  family = c("gaussian", "binomial", "multinomial", "poisson"),
  return_metrics = FALSE,
  seed = NULL,
  file = NULL,
  name = NULL
)
}
\arguments{
\item{data}{A dataframe object. This should be a structured dataset where
each column represents a variable and each row represents an observation.}

\item{composite_list}{A required \code{composite_list} object. Each name in
the list represents a composite variable, and the corresponding vector
contains the column names that are associated with the indicators
comprising said composite variable.}

\item{composite_model}{An optional \code{composite_model} object. Combines
one or more sets of `link` paths specifying directed associations between
variables. The model is assumed to be a directed acyclic graph.}

\item{weight}{Required weighting schema. Schemas include \code{c("irt",
"pca", "glm")}. \code{c("pca", "glm")} run the same weighting schema.
Default is \code{"irt"}.}

\item{pred_type}{Prediction method schema for predictive weighting. Schemas
include \code{c("glm", "rf")}. \code{"glm"} runs \code{glmnet::cv.glmnet()}
for regularization linear regression. \code{"rf"} runs
\code{ranger::ranger()} for Random Forest modeling. Default is
\code{"glm"}.}

\item{item_type}{Character string or vector specifying the item response
model(s) to be used, passed to the \code{itemtype} argument in
\code{mirt::mirt()}. See \code{?mirt::mirt} for the full list of supported
item types and their definitions. Defaults to \code{NULL}.}

\item{pmm_k}{Integer. Number of donor candidates used for predictive mean
matching (PMM) during imputation. If set to \code{0}, PMM is disabled and
raw predictions are used.}

\item{maxiter}{Integer. Maximum number of iterations for chained equations in
\code{missRanger::missRanger()}. Iteration continues until convergence or
the specified limit is reached.}

\item{verbose}{Logical; if \code{1}, print progress messages and intermediate
output. Defaults to \code{0}.}

\item{digits}{The decimal places for the metrics to be rounded to. Default is
3. This argument is only relevant if \code{return_metrics = TRUE}.}

\item{alpha}{Elastic net mixing parameter, with \code{0 <= alpha <= 1}.
Controls the relative weighting of L1 (lasso) and L2 (ridge) penalties in
the model. The penalty term is defined as \code{(1 - alpha)/2 * sum(beta^2)
+ alpha * sum(abs(beta))}, where \code{beta} is the vector of coefficients.
When \code{alpha = 1}, the penalty is pure lasso; when \code{alpha = 0}, it
is ridge regression.}

\item{nfolds}{Integer. The number of folds used for random forest k-fold
cross-validation. Controls how the data are partitioned during resampling.
Must be at least 2. Default is 10.}

\item{ntrees}{Integer. Number of trees to grow in the random forest model.
Default in \code{ranger} is 500.}

\item{importance}{Character string specifying the type of variable importance
measure to compute. Must be one of \code{"permutation"} or
\code{"impurity"}.
\itemize{
  \item \code{"permutation"}: Computes mean decrease in predictive accuracy by permuting each variable and measuring the resulting drop in model performance (typically using out-of-bag data). This method is more computationally expensive but generally provides more reliable and differentiated estimates of variable importance. It is robust to scale and less biased in the presence of correlated or irrelevant predictors.
  \item \code{"impurity"}: Computes mean decrease in node impurity (e.g., Gini for classification, variance for regression) aggregated over all trees. This method is fast and computed during model training, but can be biased toward variables with many categories or continuous values, and tends to distribute importance more evenly across predictorsâ€”even when some are irrelevant or redundant.
}
Recommend using \code{"impurity"} for speed and \code{"permutation"} for
interpretability, reliability, or feature selection. Default is
\code{"permutation"}.}

\item{family}{Character string specifying the model family. Must be one of
\code{"gaussian"}, \code{"binomial"}, \code{"multinomial"}, or
\code{"poisson"}. This argument determines the loss function used in
penalized model fitting via \code{cv.glmnet}. The supported options are:
\itemize{
  \item \code{"gaussian"}: for linear regression with continuous outcomes.
  \item \code{"binomial"}: for binary classification using logistic regression. The response should be a factor with two levels or a numeric vector containing 0 and 1.
  \item \code{"multinomial"}: for multi-class classification with three or more unordered outcome categories. The response should be a factor.
  \item \code{"poisson"}: for modeling count data under a Poisson distribution with a log link. The response should be a non-negative count-valued numeric vector.
}
Default is \code{"gaussian"}.}

\item{return_metrics}{Logic to determine whether to return reliability and
validity metrics. Set to \code{TRUE} for a list of dataframes with
reliability and validity metrics.}

\item{seed}{An integer seed for reproducibility. Defaults to \code{NULL},
where no seed is set.}

\item{file}{An optional file path. If specified, the results will be written
as a formatted excel workbook. This argument is only relevant if
\code{return_metrics = TRUE}.}

\item{name}{A required string denoting the name of the composite variable.}
}
\value{
If \code{return_metrics = FALSE}, a dataframe identical to the input
  dataframe, with additional columns appended at the end, is returned. These
  new columns represent the calculated composite scores. If
  \code{return_metrics = TRUE}, a list containing the following dataframes is
  returned:
 \itemize{
 \item \strong{Data}: A dataframe with the composite variables appended as new
 variables.
 \item \strong{Metrics}: A matrix of indicator loadings and weights metrics.
 \item \strong{Validity}: A matrix of composite reliability and validity
 metrics.
}
}
\description{
Create composite scores of scales by specifying the indicators
  that go into each respective composite variable.
}
\details{
Discriminant composite scores combine psychometric discrimination
and predictive utility into a single weighting schema. Two primary
discriminant sources are supported: \strong{latent composite loadings} and
\strong{predictive weights}. The composite score for case \eqn{c} is computed
as:

\deqn{\bar{C}_c = \frac{1}{m} \sum_{j=1}^{m} I_{cj} \cdot w_j^{*}}

where \eqn{I_{cj}} is the value of indicator \eqn{j} for case \eqn{c}, and
\eqn{w_j^{*}} is the final normalized weight. The weighting includes two
stages:

\strong{1. Discriminant weighting from latent structure.}

If \code{weight = "irt"}, a unidimensional Item Response Theory (IRT) model
is estimated:

\deqn{P(Y_{cj} = 1) = \mathrm{logit}^{-1}(a_j \cdot \theta_c + b_j)}

where \eqn{a_j} is the discrimination parameter for indicator \eqn{j}, and
\eqn{\theta_c} is the latent trait estimate for case \eqn{c}. The
discrimination parameters \eqn{a_j} are extracted and normalized:

\deqn{w_j = \frac{a_j}{\sum_{k=1}^{m} a_k}}

If \code{weight = "glm"} or \code{"pca"}, a single-factor principal component
analysis (PCA) is conducted. The first factor score is extracted as a latent
proxy \eqn{Z_c}. A penalized linear model is then fit:

\deqn{Z_c = \sum_{j=1}^{m} \beta_j \cdot I_{cj} + \varepsilon_c}

where \eqn{\beta_j} are estimated using elastic net regularization (via
\pkg{glmnet}). These are normalized analogously to IRT:

\deqn{w_j = \frac{\beta_j}{\sum_{k=1}^{m} \beta_k}}

\strong{2. Predictive weighting from external outcomes (optional).}

If one or more outcome variables are provided via \code{outcomes}, an
additional predictive weighting stage is applied. Two methods are available:

\emph{(a) Generalized Linear Model (GLM).} Each outcome \eqn{Y} is regressed
on the indicators \eqn{I_{cj}} using elastic net regularization. For each
fitted model, the normalized coefficients \eqn{\gamma_j^{(r)}} are extracted.
The final predictive weights are averaged over all outcomes:

\deqn{p_j = \frac{1}{R} \sum_{r=1}^{R} \gamma_j^{(r)}}

where \eqn{R} is the number of outcomes. The predictive weights are then
normalized:

\deqn{p_j^{*} = \frac{p_j}{\sum_{k=1}^{m} p_k}}

\emph{(b) Random Forest (RF).} Each outcome \eqn{Y} is predicted via a random
forest, and variable importance scores (e.g., permutation or impurity) are
extracted for each indicator. These are averaged across outcomes and
normalized in the same manner as the GLM approach (see step a above).

\strong{3. Final weight aggregation.}

If predictive weights \eqn{p_j^{*}} are available, they are combined with the
latent discrimination weights \eqn{w_j} to form final composite weights:

\deqn{w_j^{*} = \frac{w_j + p_j^{*}}{\frac{1}{m} \sum_{k=1}^{m} (w_k +
p_k^{*})}}

If no predictive targets are provided, \eqn{w_j^{*}} defaults to normalized
discrimination weights.
}
\examples{

data(grit)

# Specify the named list with composite names and their respective indicators
composite_list <- composite_list(

  # Lower-order composites
  extraversion          = sprintf("e\%01d", seq(10)),
  neuroticism           = sprintf("n\%01d", seq(10)),
  agreeableness         = sprintf("a\%01d", seq(10)),
  conscientiousness     = sprintf("c\%01d", seq(10)),
  openness              = sprintf("o\%01d", seq(10)),
  consistency_interest  = sprintf("gs\%01d", c(2,3,5,7,8,11)),
  perseverance_effort   = sprintf("gs\%01d", c(1,4,6,9,10,12)),

  # Higher-order composites
  grit                  = c("consistency_interest", "perseverance_effort")

 )

# Calculate discriminant-weighted composite scores
discriminant_score(data = grit,
                   composite_list = composite_list)

# Calculate discriminant-weighted composite scores, reliability, & validity
discriminant_score(data = grit,
                   composite_list = composite_list,
                   digits = 3,
                   return_metrics = TRUE,
                   file = "composite.xlsx")

unlink("composite.xlsx")


}
