% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/score_information.R
\name{information_score}
\alias{information_score}
\title{Calculate Information-Weighted Composite Scores}
\usage{
information_score(
  data = .,
  composite_list,
  entropy = c("emp", "mm", "shrink", "sg"),
  nmi_method = c("geometric", "average"),
  threshold = 10,
  digits = 3,
  return_metrics = FALSE,
  file = NULL,
  name = NULL
)
}
\arguments{
\item{data}{A dataframe object. This should be a structured dataset where
each column represents a variable and each row represents an observation.}

\item{composite_list}{A required \code{composite_list} object. Each name in
the list represents a composite variable, and the corresponding vector
contains the column names that are associated with the indicators
comprising said composite variable.}

\item{entropy}{A string value reflecting the mutual information entropy
estimator from the \code{infotheo} package. Four estimators are available:
\code{emp} to compute the entropy of the empirical probability
distribution. Empirical entropy is suitable for simple calculations without
corrections. \code{mm} applies an asymptotic bias-corrected estimator
making it suitable for small sample sizes. \code{shrink} applies a
shrinkage estimate of the Dirichlet probability distribution to provide a
stable estimate useful for small sample sizes or sparse data. \code{sg}
applies a Schurmann-Grassberger estimate of the Dirichlet probability
distribution to serve as an alternative to the Shrinkage approach.}

\item{nmi_method}{A string value reflecting the method used for calculating
Normalized Mutual Information (NMI) values. \code{"average"} will normalize
MI values using the average entropies of variables A and B.
\code{"geometric"} will normalize MI values using the geometric mean of
entropies of variables A and B.}

\item{threshold}{An integer specifying the maximum number of unique values to
consider the input as discrete. Defaults to 10.}

\item{digits}{The decimal places for the metrics to be rounded to. Default is
3. This argument is only relevant if \code{return_metrics = TRUE}.}

\item{return_metrics}{Logic to determine whether to return reliability and
validity metrics. Set to \code{TRUE} for a list of dataframes with
reliability and validity metrics.}

\item{file}{An optional file path. If specified, the results will be written
as a formatted excel workbook. This argument is only relevant if
\code{return_metrics = TRUE}.}

\item{name}{A required string denoting the name of the composite variable.}
}
\value{
If \code{return_metrics = FALSE}, a dataframe identical to the input
  dataframe, with additional columns appended at the end, is returned. These
  new columns represent the calculated composite scores. If
  \code{return_metrics = TRUE}, a list containing the following dataframes is
  returned:
 \itemize{
 \item \strong{Data}: A dataframe with the composite variables appended as new
 variables.
 \item \strong{Metrics}: A matrix of indicator loadings and weights metrics.
 \item \strong{Validity}: A matrix of composite reliability and validity
 metrics.
}
}
\description{
Create composite scores of scales by specifying the indicators
  that go into each respective composite variable.
}
\details{
Composite scores are computed as the \strong{information-weighted}
  mean of the indicators.

  \emph{Information-weighted scores.} For a given composite, pairwise mutual
  information (MI) is computed between all indicators to form an information
  matrix \eqn{I_{ij}}. Self-information values on the diagonal are excluded.
  Each mutual information value is then normalized to obtain the Normalized
  Mutual Information (NMI).

  The NMI between variables \eqn{i} and \eqn{j} can be computed using either
  arithmetic mean:

  \deqn{\mathrm{NMI}(i, j) = \frac{2 \cdot I(i, j)}{H(i) + H(j)}}

  or geometric mean:

  \deqn{\mathrm{NMI}(i, j) = \frac{I(i, j)}{\sqrt{H(i) \cdot H(j)}}}

  where \eqn{I(i, j)} is the mutual information and \eqn{H(i)} and \eqn{H(j)}
  are the entropies of variables \eqn{i} and \eqn{j}, respectively. The
  method is selected via \code{nmi_method}. The information weight for
  indicator \eqn{j} is the average of its NMI values:

  \deqn{w_j = \frac{1}{n} \sum_{i \neq j} \mathrm{NMI}(i, j)}

  where \eqn{n} is the number of indicators and the sum excludes self-pairs.
  The weights are then normalized:

  \deqn{w_j^{*} = \frac{w_j}{\frac{1}{m} \sum_{k=1}^{m} w_k}}

  where \eqn{m} is the number of indicators in the composite. The
  information-weighted composite score \eqn{\bar{C}_{c}^{\mathrm{MI}}} for
  case \eqn{c} is calculated as:

  \deqn{\bar{C}_{c}^{\mathrm{MI}} = \frac{1}{m} \sum_{j=1}^{m} I_{cj} \cdot w_j^{*}}

  where \eqn{I_{cj}} is the value of indicator \eqn{j} for case \eqn{c}.
}
\examples{

data(grit)

# Specify the named list with composite names and their respective indicators
composite_list <- composite_list(

  # Lower-order composites
  extraversion          = sprintf("e\%01d", seq(10)),
  neuroticism           = sprintf("n\%01d", seq(10)),
  agreeableness         = sprintf("a\%01d", seq(10)),
  conscientiousness     = sprintf("c\%01d", seq(10)),
  openness              = sprintf("o\%01d", seq(10)),
  consistency_interest  = sprintf("gs\%01d", c(2,3,5,7,8,11)),
  perseverance_effort   = sprintf("gs\%01d", c(1,4,6,9,10,12)),

  # Higher-order composites
  grit                  = c("consistency_interest", "perseverance_effort")

 )

# Calculate correlation-weighted composite scores
information_score(data = grit,
                  composite_list = composite_list)

# Calculate correlation-weighted composite scores, reliability, & validity
information_score(data = grit,
                  composite_list = composite_list,
                  digits = 3,
                  return_metrics = TRUE,
                  file = "composite.xlsx")

unlink("composite.xlsx")

}
